{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clas = pd.read_csv('classification.csv')#header=None)\n",
    "scores = pd.read_csv('scores.csv')#header=None)\n",
    "print(clas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=[0,0,0,0]\n",
    "for i in range(len(clas)):\n",
    "    c=str(clas['true'][i])+str(clas['pred'][i])\n",
    "    a[eval('0b' + c)]+=1\n",
    "TP=a[3]\n",
    "FN=a[2]\n",
    "FP=a[1]\n",
    "TN=a[0]\n",
    "print(TP, FN, FP, TN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc=metrics.accuracy_score(clas['true'],clas['pred'])\n",
    "pre=metrics.precision_score(clas['true'],clas['pred'])\n",
    "rec=metrics.recall_score(clas['true'],clas['pred'])\n",
    "f1=metrics.f1_score(clas['true'],clas['pred'])\n",
    "print(acc, pre, rec, f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(scores)\n",
    "names=['score_logreg','score_svm','score_knn','score_tree']\n",
    "roc_auc=[roc_auc_score(scores['true'], scores['score_logreg']),roc_auc_score(scores['true'], scores['score_svm']),\n",
    "         roc_auc_score(scores['true'], scores['score_knn']),roc_auc_score(scores['true'], scores['score_tree'])]\n",
    "print(names[roc_auc.index(max(roc_auc))],max(roc_auc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     true  pred\n",
      "0       1     0\n",
      "1       1     1\n",
      "2       1     1\n",
      "3       0     0\n",
      "4       1     1\n",
      "..    ...   ...\n",
      "195     0     0\n",
      "196     0     0\n",
      "197     1     0\n",
      "198     0     1\n",
      "199     0     0\n",
      "\n",
      "[200 rows x 2 columns]\n",
      "43 59 34 64\n",
      "0.535 0.5584415584415584 0.4215686274509804 0.48044692737430167\n",
      "     true  score_logreg  score_svm  score_knn  score_tree\n",
      "0       0      0.683832   0.145976   0.787063    0.500000\n",
      "1       1      0.801966   0.239511   1.000000    0.833333\n",
      "2       0      0.382315  -0.245701   0.000000    0.000000\n",
      "3       1      0.506797  -0.137058   0.000000    0.105263\n",
      "4       1      0.488781  -0.154148   0.000000    0.105263\n",
      "..    ...           ...        ...        ...         ...\n",
      "195     0      0.573801  -0.088203   0.284192    0.400000\n",
      "196     0      0.624422  -0.012315   0.205437    0.400000\n",
      "197     1      0.425538  -0.135673   0.382351    0.700000\n",
      "198     0      0.905270   0.583806   1.000000    1.000000\n",
      "199     0      0.275594  -0.422160   0.743567    0.642857\n",
      "\n",
      "[200 rows x 5 columns]\n",
      "score_logreg 0.719187675070028\n",
      "[0.6302521008403361, 0.6228070175438597, 0.6065573770491803, 0.6517857142857143]\n",
      "score_tree 0.6517857142857143\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds=metrics.precision_recall_curve(scores['true'], scores['score_logreg'])\n",
    "data1=pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "data1=data1[data1['recall']>=0.7]\n",
    "\n",
    "precision, recall, thresholds=metrics.precision_recall_curve(scores['true'], scores['score_svm'])\n",
    "data2=pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "data2=data2[data2['recall']>=0.7]\n",
    "\n",
    "precision, recall, thresholds=metrics.precision_recall_curve(scores['true'], scores['score_knn'])\n",
    "data3=pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "data3=data3[data3['recall']>=0.7]\n",
    "\n",
    "precision, recall, thresholds=metrics.precision_recall_curve(scores['true'], scores['score_tree'])\n",
    "data4=pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "data4=data4[data4['recall']>=0.7]\n",
    "\n",
    "result=[data1['precision'].max(),data2['precision'].max(),data3['precision'].max(),data4['precision'].max()]\n",
    "print(result)\n",
    "print(names[result.index(max(result))],max(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}